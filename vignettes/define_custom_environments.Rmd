---
title: "Define custom environment for deep reinforcement learn"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: false
    dev: svg
vignette: >
  %\VignetteIndexEntry{Define custom environment for deep reinforcement learn}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE, cache = FALSE}
library(rlR)
set.seed(123)
knitr::opts_chunk$set(cache = TRUE, collapse = FALSE, dev = "svg", fig.height = 3.5)
knitr::knit_hooks$set(document = function(x){
  gsub("```\n*```r*\n*", "", x)
})
library(reticulate)
os = import("os")
os$environ[["TF_CPP_MIN_LOG_LEVEL"]]="3"
```

# rlR: Define custom environments for Deep Reinforcement learning in R

## Environment class
If you want to use this package for your self defined task, you need to implement your own R6 class to represent the environment. Below is a template. Where the listed fields (member variables and methods) must exist in your implemented R6 class. You could define other public and private members as you like. 

```{r}
library(rlR)
library(R6)
MyEnv = R6::R6Class("MyEnv",
  inherit = Environment,
  public = list(
    state_dim = NULL,  # obligatory field, should be vector. c(28, 28, 3) which is usually the dimension for an order 3 tensor which can represent an RGB image for example.
    act_cnt = NULL, # obligatory field, should be type integer
    s_r_d_info = NULL,
    step_cnt = NULL,
    initialize = function(...) {
      # put your initialization code here for example
      self$flag_continous = FALSE  # non-continuous action
      self$state_dim = 4
      self$act_cnt = 2
      self$step_cnt = 0L
      self$s_r_d_info = vector(mode = "list", length = 4)
      names(self$s_r_d_info) = c("state", "reward", "done", "info")
      self$s_r_d_info[["reward"]] = 1  # design your own reward scheme in step function below
      self$s_r_d_info[["done"]] = FALSE  # whether the episode is finished?
      self$s_r_d_info[["state"]] = array(0, dim = self$state_dim) # state must be array of the same dimension as the self$state_dim
      self$s_r_d_info[["info"]] = list()  # info can be arbitray object
      as.array
    },

    render = function(...) {
      # you could leave this field empty. 
    },

    # this function will be called at each step of the learning
    step = function(action) {
      # input your custom code here
      self$step_cnt = self$step_cnt + 1L
      self$s_r_d_info[["reward"]] = rnorm(1L)  # design your own reward scheme in step function below
      if(self$step_cnt > 5L)  self$s_r_d_info[["done"]] = TRUE
      self$s_r_d_info
    },

    # this function will be called at the beginning of the learning and at the end of each episode
    reset = function() {
      # input your custom function FALSE
      self$step_cnt = 0
      self$s_r_d_info[["done"]] = FALSE
      self$s_r_d_info
    },

    afterAll = function() {
      # what to do after the whole learning is finished?  could be left empty
    }
    ),
  private = list(),
  active = list()
  )
```

Afterwards you could choose one of the available  Agents to learn on this newly defined environments. 
```{r}
env = MyEnv$new()
listAvailAgent()
agent = makeAgent("AgentDQN", env)
agent$updatePara("console", FALSE)
perf = agent$learn(10)
```

